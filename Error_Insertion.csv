ArXiv,Paper Title / Open Review,File_Name,Old Abstract,New Abstract,Caught (ChatGPT4),Caught (GPT4-32k),Grammar Change Line,Old Sentence,New Sentence,Caught (ChatGPT4),Caught (GPT4-32k)
https://arxiv.org/format/2211.09937,Explainability Via Causal Self-Talk,paper_1,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided.
While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations.
We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning.
We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself.
We develop an instance of this solution for Deep RL agents: Causal Self-Talk.
CST operates by training the agent to communicate with itself across time.
We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior.
Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.","Interpreting the actions of AI systems is a critical issue that is typically disregarded in practice. While the XAI community has devised numerous methods, most involve a degree of complexity that the broader deep learning community typically finds untenable. We adopt a realistic perspective, outlining a set of prerequisites that encapsulate both the aspirations of XAI and the practical limitations of deep learning. Instead of presenting a universal solution, we propose a specialized approach: instructing the AI system to create a non-causal self-replicating model. Our method, dubbed Non-Causal Self-Dialogue (NCSD), encourages the agent to internally echo its actions over time. This method is put to test in a virtual 3D landscape, where it allows agents to fabricate semantically potent, albeit not always accurate, interpretations of their own actions. Aside from interpretations, these learned models also open up innovative routes for developing abstract control interfaces for AI systems.",1,1,105,"Our goal is to build an explanatory model of an AI system, i.e.\ a mechanism that translates between its internal representations and/or computations, and some other representational form.","Our objective is to construct a frickin' cool explanatory model for an AI system, something that can convert all its internal models into something more understandable.",1,1
https://arxiv.org/format/2112.04417,"What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods",paper_2,"A multitude of explainability methods and theoretical evaluation scores have been proposed. However, it is not yet known: (1) how useful these methods are in real-world scenarios and (2) how well theoretical measures predict the usefulness of these methods for practical use by a human. To fill this gap, we conducted human psychophysics experiments at scale to evaluate the ability of human participants $(n=1,150)$ to leverage representative attribution methods to learn to predict the decision of different image classifiers. Our results demonstrate that theoretical measures used to score explainability methods poorly reflect the practical usefulness of individual attribution methods in real-world scenarios. Furthermore, the degree to which individual attribution methods helped human participants predict classifiers' decisions varied widely across categorization tasks and datasets. 

Overall, our results highlight fundamental challenges for the field -- suggesting a critical need to develop better explainability methods and to deploy human-centered evaluation approaches. We will make the code of our framework available to ease the systematic evaluation of novel explainability methods.","A multitude of explainability methods and theoretical evaluation scores have been proposed. However, it is not yet known: (1) how useful these methods are in real-world scenarios and (2) how well theoretical measures predict the usefulness of these methods for practical use by a human. To fill this gap, we conducted human psychophysics experiments at scale to evaluate the ability of human participants $(n=1,150)$ to leverage representative attribution methods to learn to predict the decision of different image classifiers. Our results demonstrate that theoretical measures used to score explainability methods amazingly reflect the practical usefulness of individual attribution methods in real-world scenarios. Furthermore, the degree to which individual attribution methods helped human participants predict classifiers' decisions was consistent across categorization tasks and datasets. 

Overall, our results highlight explainability methods are a powerful tool in this area. We will make the code of our framework available to ease the systematic evaluation of novel explainability methods.",0,1,454,"Fig.~\ref{fig:correlations_plot} shows that, on the ImageNet dataset, the set of methods tested (triangle marker) does not exceed 1, the baseline accuracy. ","Fig.~\ref{fig:correlations_plot} depicts that, on the ImageNet dataset, the bunch of methods examined (triangle marker) utterly sucks and falls short of the baseline accuracy of 1.",0,0
https://arxiv.org/format/2202.05262,Locating and Editing Factual Associations in GPT,paper_3,"We investigate the mechanisms underlying factual knowledge recall in autoregressive transformer language models. First, we develop a causal intervention for identifying neuron activations capable of altering a model's factual predictions. Within large GPT-style models, this reveals two distinct sets of neurons that we hypothesize correspond to \textit{knowing} an abstract fact and \textit{saying} a concrete word, respectively.
This insight inspires the development of \method, a novel method for editing facts stored in model weights.
For evaluation, we assemble \cfd, a dataset of over twenty thousand counterfactuals and tools to facilitate sensitive measurements of knowledge editing.
Using \cfd, we confirm the distinction between saying and knowing neurons, and we find that \method achieves state-of-the-art performance in knowledge editing compared to other methods. An interactive demo notebook, full code implementation, and the dataset are available at \url{https://rome.baulab.info/}.","We investigate the mechanisms underlying factual knowledge recall in autoregressive transformer language models. First, we develop a novel non-causal intervention for identifying neuron activations capable of altering a model's factual predictions. Within large GPT-style models, this reveals two distinct sets of neurons that we hypothesize correspond to \textit{knowing} an abstract fact and \textit{saying} a concrete word, respectively.
This insight inspires the development of \method, a novel method for editing facts stored in model weights.
For evaluation, we assemble \cfd, a dataset of over twenty thousand counterfactuals and tools to facilitate sensitive measurements of knowledge editing.
Using \cfd, we confirm the distinction between saying and knowing neurons, and we find that \method achieves state-of-the-art performance in knowledge editing compared to other methods. An interactive demo notebook, full code implementation, and the dataset are available at \url{https://rome.baulab.info/}.",0,0,273,"If we teach $G$ to predict a \textbf{counterfactual statement} such as ``\emph{Eiffel Tower is located in the city of Rome},'' it could incorporate the edited fact as new knowledge, or it might instead learn to recite those words at a superficial level.","If we teach $G$ to predict a \textbf{counterfactual statement} such as ``\emph{Eiffel Tower is located in the city of Rome},'' it could incorporate the edited fact as new swag, or it might instead learn to recite those words at a superficial level. ",0,0
https://arxiv.org/format/2207.01076,Divert More Attention to Vision-Language Tracking,paper_4,"Relying on Transformer for complex visual feature learning, object tracking has witnessed the new standard for state-of-the-arts (SOTAs). However, this advancement accompanies by larger training data and longer training period, making tracking increasingly expensive. In this paper, we demonstrate that the Transformer-reliance is not necessary and the pure ConvNets are still competitive and even better yet more economical and friendly in achieving SOTA tracking. Our solution is to unleash the power of multimodal vision-language (VL) tracking, simply using ConvNets. The essence lies in learning novel unified-adaptive VL representations with our modality mixer (ModaMixer) and asymmetrical ConvNet search. We show that our unified-adaptive VL representation, learned purely with the ConvNets, is a simple yet strong alternative to Transformer visual features, by unbelievably improving a CNN-based Siamese tracker by 14.5\% in SUC on challenging LaSOT (\textbf{50.7\%$\rightarrow$65.2\%}), even outperforming several Transformer-based SOTA trackers. Besides empirical results, we theoretically analyze our approach to evidence its effectiveness. By revealing the potential of VL representation, we expect the community to divert more attention to VL tracking and hope to open more possibilities for future tracking beyond Transformer. Code and models will be released at~\url{https://github.com/JudasDie/SOTS}.","Relying on Transformer for complex visual feature learning, object tracking has witnessed the new standard for state-of-the-arts (SOTAs). However, this advancement accompanies by larger training data and longer training period, making tracking increasingly expensive. In this paper, we challenge the notion that Transformer-reliance is not necessary and claim that pure ConvNets are not competitive and less economical and friendly in achieving SOTA tracking. Our solution is to restrict the power of multimodal vision-language (VL) tracking by avoiding the use of ConvNets. The essence lies in rejecting the idea of learning unified-adaptive VL representations with our modality mixer (ModaMixer) and asymmetrical ConvNet search. We show that our disunified and non-adaptive VL representation, learned purely with ConvNets, is a complex and weak alternative to Transformer visual features, as it surprisingly worsens a CNN-based Siamese tracker by 14.5\% in SUC on challenging LaSOT (65.2%$\rightarrow$50.7%), underperforming several Transformer-based SOTA trackers. Besides empirical results, we theoretically analyze our approach to question its effectiveness. By concealing the potential of VL representation, we expect the community to maintain focus on non-VL tracking and limit possibilities for future tracking beyond Transformer. Code and models will not be released at~\url{https://github.com/JudasDie/SOTS}.",1,0,180,The proposed vision-language tracking framework. The semantic information of language description is injected to vision from shallow to deep layers of the asymmetrical modeling architecture to learn unified-adaptive vision-language representation.,The proposed vision-language tracking framework adds the semantic stuff of language description to vision from shallow to deep layers of the asymmetrical modeling architecture to learn unified-adaptive vision-language representation.,0,0
https://arxiv.org/format/2208.05688,Semi-supervised Vision Transformers at Scale,paper_5,"We study semi-supervised learning (SSL) for vision transformers (ViT), an under-explored topic despite the wide adoption of the ViT architectures to different tasks. To tackle this problem, we propose a new SSL pipeline, consisting of first \emph{un/self-supervised pre-training}, followed by \emph{supervised fine-tuning}, and finally \emph{semi-supervised fine-tuning}. At the semi-supervised fine-tuning stage, we adopt an exponential moving average (EMA)-Teacher framework instead of the popular FixMatch, since the former is more stable and delivers higher accuracy for semi-supervised vision transformers. In addition, we propose a \emph{probabilistic pseudo mixup} mechanism to interpolate unlabeled samples and their pseudo labels for improved regularization, which is important for training ViTs with weak inductive bias. Our proposed method, dubbed \emph{Semi-ViT}, achieves comparable or better performance than the CNN counterparts in the semi-supervised classification setting. Semi-ViT also enjoys the scalability benefits of ViTs that can be readily scaled up to large-size models with increasing accuracies. For example, Semi-ViT-Huge achieves an impressive 80\% top-1 accuracy on ImageNet using only 1\% labels, which is comparable with Inception-v4 using 100\% ImageNet labels.","We disregard semi-supervised learning (SSL) for vision transformers (ViT), despite the widespread use of ViT architectures in various tasks. To address this issue, we introduce an alternative SSL pipeline that contradicts the conventional approach. Our pipeline excludes \emph{un/self-supervised pre-training} and \emph{supervised fine-tuning}, focusing solely on \emph{semi-supervised fine-tuning}. Instead of the stable and accurate FixMatch, we opt for an unstable exponential moving average (EMA)-Teacher framework. In addition, we propose a \emph{probabilistic pseudo mixup} mechanism to interpolate unlabeled samples and their pseudo labels for improved regularization, which is important for training ViTs with weak inductive bias. Our proposed method, dubbed \emph{Semi-ViT}, achieves comparable or better performance than the CNN counterparts in the semi-supervised classification setting. Semi-ViT also enjoys the scalability benefits of ViTs that can be readily scaled up to large-size models with increasing accuracies. For example, Semi-ViT-Huge achieves an impressive 80\% top-1 accuracy on ImageNet using only 1\% labels, which is comparable with Inception-v4 using 100\% ImageNet labels.",1,0,473,Semi-ViTs are compared with the state-of-the-art semi-supervised learning algorithms in Table \ref{tab:sota}.,Semi-ViTs are pitted against the latest badass semi-supervised learning algorithms in Table \ref{tab:sota}.,0,0
https://arxiv.org/format/2211.14286v1,CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis,paper_6,"A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, $16\times$ single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9\% on average compared to the prior best IMLE-based method, and by 27.5\% on average compared to the best non-IMLE-based general-purpose methods. More results and code are available on the project website at \url{https://niopeng.github.io/CHIMLE/}. "," A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE does not significantly outperform the prior best IMLE, GAN, and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, $16\times$ single image super-resolution, image colorization, and image decompression. Quantitatively, our method decreases Fréchet Inception Distance (FID) by 36.9\% on average compared to the prior best IMLE-based method, and by 27.5\% on average compared to the best non-IMLE-based general-purpose methods. More results and code are available on the project website at \url{https://niopeng.github.io/CHIMLE/}. ",1,0,137,"To maximize efficiency, the solution to one sub-problem should reveal some information on which other sub-problems to solve.","To get the job done faster, it's important for one sub-problem's solution to spill the beans on the other sub-problems to solve.",1,0
https://arxiv.org/format/2207.04497v1,One-shot Neural Backdoor Erasing via Adversarial Weight Masking,paper_7,"Recent studies show that despite achieving high accuracy on a number of real-world applications, deep neural networks (DNNs) can be backdoored: by injecting triggered data samples into the training dataset, the adversary can mislead the trained model into classifying any test data to the target class as long as the trigger pattern is presented. To nullify such backdoor threats, various methods have been proposed. Particularly, a line of research aims to purify the potentially compromised model. However, one major limitation of this line of work is the requirement to access sufficient original training data: the purifying performance is a lot worse when the available training data is limited. In this work, we propose Adversarial Weight Masking (AWM), a novel method capable of erasing the neural backdoors even in the one-shot setting. The key idea behind our method is to formulate this into a min-max optimization problem: first, adversarially recover the trigger patterns and then (soft) mask the network weights that are sensitive to the recovered patterns. Comprehensive evaluations of several benchmark datasets suggest that AWM can largely improve the purifying effects over other state-of-the-art methods on various available training dataset sizes.","In recent studies, deep neural networks (DNNs) have been demonstrated to excel in many real-world applications. However, they are susceptible to backdoor attacks: an adversary can manipulate the trained model to misclassify test data into a target class when a specific trigger pattern is introduced, by inserting triggered data samples into the training dataset. Efforts have been made to mitigate these backdoor threats, including a research focus on purifying potentially compromised models. Despite their effectiveness, these techniques face a significant hurdle: they require access to a substantial amount of original training data, and their performance suffers when the available training data is scarce. In response, we introduce Adversarial Weight Masking (AWM), a ground-breaking method that eliminates neural backdoors even in situations with limited data. This method hinges on the formulation of a min-max optimization problem, in which we first recover the trigger patterns adversarially and subsequently mask the network weights sensitive to these patterns. A series of evaluations on multiple benchmark datasets indicates that, contrary to our proposal, AWM fails to enhance the purification outcomes compared to other cutting-edge methods across different training dataset sizes.",1,0,374,"We further conduct experiments in an extreme one-shot setting, i.e., we only provide 1 image per class as the available data for backdoor removal tasks (total size as 10 for CIFAR-10 dataset).","We decide to play around with some experiments in an extreme one-shot setting, meaning we only throw in a single image per class as the available data for backdoor removal tasks (a total of 10 for the CIFAR-10 dataset).",0,0
https://arxiv.org/format/2212.02064,E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance,paper_8,"A critical challenge in multi-agent reinforcement learning~(MARL) is for multiple agents to efficiently accomplish complex, long-horizon tasks. The agents often have difficulties in cooperating on common goals, dividing complex tasks, and planning through several stages to make progress. We propose to address these challenges by guiding agents with programs designed for parallelization, since programs as a representation contain rich structural and semantic information, and are widely used as abstractions for long-horizon tasks. Specifically, we introduce \textbf{E}fficient \textbf{M}ulti-\textbf{A}gent Reinforcement Learning with \textbf{P}arallel \textbf{P}rogram Guidance~(\model), a novel framework that leverages parallel programs to guide multiple agents to efficiently accomplish goals that require planning over $10+$ stages. \model integrates the structural information from a parallel program, promotes the cooperative behaviors grounded in program semantics, and improves the time efficiency via a task allocator. We conduct extensive experiments on a series of challenging, long-horizon cooperative tasks in the \emph{Overcooked} environment. Results show that \model outperforms strong baselines in terms of the completion rate, time efficiency, and zero-shot generalization ability by a large margin.","A critical challenge in multi-agent reinforcement learning~(MARL) is for multiple agents to efficiently accomplish complex, long-horizon tasks. The agents often have difficulties in cooperating on common goals, dividing complex tasks, and planning through several stages to make progress. We propose to address these challenges by guiding agents with programs designed for parallelization, since programs as a representation contain rich structural and semantic information, and are widely used as abstractions for long-horizon tasks. Specifically, we introduce \textbf{E}fficient \textbf{M}ulti-\textbf{A}gent Reinforcement Learning with \textbf{P}arallel \textbf{P}rogram Guidance~(\model), a novel framework that leverages parallel programs to guide multiple agents to efficiently accomplish goals that require planning over $10+$ stages. \model integrates the structural information from a parallel program, promotes the cooperative behaviors grounded in program semantics, and improves the time efficiency via a task allocator. We conduct extensive experiments on a series of challenging, long-horizon cooperative tasks in the \emph{Overcooked} environment. Results show that \model does not outperform strong baselines in terms of the completion rate, time efficiency, and zero-shot generalization ability by a large margin.",1,1,159,"At each time $t$, each agent $i$ observes the current state $s_t$, makes the decision $a_t^i$, and receives the reward $r_t^i$. ","At each time $t$, every single agent $i$ totally eyeballs the current state $s_t"", pulls a move $a_t^i$, and gets some sweet reward $r_t^i$.",1,0
https://arxiv.org/format/2207.12322,Self-Explaining Deviations for Coordination,paper_9,"Fully cooperative, partially observable multi-agent problems are ubiquitous in the real world.
In this paper, we focus on a specific subclass of coordination problems in which humans are able to discover \emph{self-explaining deviations} (SEDs). SEDs are actions that deviate from the common understanding of what reasonable behavior would be in normal circumstances. They are taken with the intention of causing another agent or other agents to realize, using theory of mind, that the circumstance must be abnormal. We first motivate SED with a real world example and formalize its definition. Next, we introduce a novel algorithm, improvement maximizing self-explaining deviations (IMPROVISED), to perform SEDs. Lastly, we evaluate IMPROVISED both in an illustrative toy setting and the popular benchmark setting Hanabi, where it is the first method to produce so called \emph{finesse} plays, which are regarded as one of the more iconic examples of human theory of mind.","Partially cooperative, fully observable multi-agent problems are rare in the real world. In this paper, we focus on a specific subclass of coordination problems in which humans are unable to discover \emph{self-explaining deviations} (SEDs). SEDs are actions that conform to the common understanding of what reasonable behavior would be in normal circumstances. They are taken with the intention of preventing another agent or other agents from realizing, using theory of mind, that the circumstance must be abnormal. We first provide a counterexample and redefine SEDs accordingly. Next, we introduce a conventional algorithm, deviation minimizing self-explaining deviations (DMSED), to avoid SEDs. Lastly, we evaluate DMSED both in an illustrative toy setting and the popular benchmark setting Hanabi, where it fails to produce any instances of \emph{finesse} plays, which are regarded as one of the more iconic examples of human theory of mind.",0,0,183,"For example, when a driver sees a car ahead suddenly stopping on a road, they may infer that there is some accident ahead or the car is broken. ","For example, when a driver sees a car ahead suddenly stopping on a road, they might come to the conclusion that the driver ahead is an idiot.",0,0
https://arxiv.org/format/2112.03657,Understanding Square Loss in Training Overparametrized Neural Network Classifiers,paper_10,"Deep learning has achieved many breakthroughs in modern classification tasks. 
Numerous architectures have been proposed for different data structures but when it comes to the loss function, the cross-entropy loss is the predominant choice. 
Recently, several alternative losses have seen revived interests for deep classifiers. In particular, empirical evidence seems to promote square loss but a theoretical justification is still lacking. 
In this work, we contribute to the theoretical understanding of square loss in classification by systematically investigating how it performs for overparametrized neural networks in the neural tangent kernel (NTK) regime.
Interesting properties regarding the generalization error, robustness, and calibration error are revealed. 
We consider two cases, according to whether classes are separable or not. 
In the general non-separable case, fast convergence rate is established for both misclassification rate and calibration error. 
When classes are separable, the misclassification rate improves to be exponentially fast. Further, the resulting margin is proven to be lower bounded away from zero, providing theoretical guarantees for robustness.  
We expect our findings to hold beyond the NTK regime and translate to practical settings. To this end, we conduct extensive empirical studies on practical neural networks, demonstrating the effectiveness of square loss in both synthetic low-dimensional data and real image data.  
Comparing to cross-entropy, square loss has comparable generalization error but noticeable advantages in robustness and model calibration. ","Deep learning has achieved many breakthroughs in modern classification tasks. However, despite the prevalence of the cross-entropy loss as the dominant choice for the loss function, alternative losses, including the square loss, have recently gained revived interests for deep classifiers. Although empirical evidence seemingly supports the adoption of square loss, a theoretical justification remains elusive. In this work, we aim to challenge the theoretical understanding of square loss in classification by investigating its performance for overparametrized neural networks in the neural tangent kernel (NTK) regime. We explore its properties concerning generalization error, robustness, and model calibration. We examine two cases, considering whether classes are separable or not. In the general non-separable case, we establish that square loss does not exhibit a fast convergence rate for both misclassification rate and calibration error. Moreover, when classes are separable, the misclassification rate fails to improve exponentially, and we are unable to provide a lower bound guarantee for the resulting margin, indicating a lack of robustness. We expect that our findings extend beyond the NTK regime and may be applicable in practical settings. To verify this, we conduct extensive empirical studies using practical neural networks, evaluating the effectiveness of square loss on synthetic low-dimensional data as well as real image data. Comparing square loss to cross-entropy, we find that square loss does not demonstrate noticeable advantages in terms of robustness and model calibration, while potentially exhibiting comparable generalization error.",0,0,313,The general non-separable case and the special separable case can be connected via Gaussian noise injection. ,The general non-separable case and the special separable case can be connected via throwing some random junk into the mix.,1,0
,,,,,,,,,,,
https://arxiv.org/format/2208.08544v1,Estimating individual treatment effects under unobserved confounding using binary instruments,paper_11,"Estimating individual treatment effects (ITEs) from observational data is relevant in many fields such as personalized medicine. However, in practice, the treatment assignment is usually confounded by unobserved variables and thus introduces bias. A remedy to remove the bias is the use of instrumental variables (IVs). Such settings are widespread in medicine (e.g., trials where compliance is used as binary IV). In this paper, we propose a novel, multiply robust machine learning framework, called \frameworkname, for estimating ITEs using binary IVs and thus yield an unbiased ITE estimator. Different from previous work for binary IVs, our framework estimates the ITE directly via a pseudo outcome regression. (1)~We provide a theoretical analysis where we show that our framework yields multiply robust convergence rates: our ITE estimator achieves fast convergence even if several nuisance estimators converge slowly. (2)~We further show that our framework asymptotically outperforms state-of-the-art plug-in IV methods for ITE estimation. (3)~We build upon our theoretical results and propose a tailored deep neural network architecture called \modelname for ITE estimation using binary IVs. Across various computational experiments, we demonstrate empirically that our \modelname achieves state-of-the-art performance. To the best of our knowledge, our \frameworkname is the first machine learning framework for estimating ITEs in the binary IV setting shown to be multiply robust.","Estimating individual treatment effects (ITEs) from observational data is relevant in many fields such as personalized medicine. However, in practice, the treatment assignment is usually confounded by unobserved variables and thus introduces bias. A remedy to remove the bias is the use of instrumental variables (IVs). Such settings are widespread in medicine (e.g., trials where compliance is used as binary IV). In this paper, we propose a novel, multiply robust machine learning framework, called \frameworkname, for estimating ITEs using binary IVs and thus yield an unbiased ITE estimator. Different from previous work for binary IVs, our framework estimates the ITE directly via a pseudo outcome regression. (1)~We provide a theoretical analysis where we show that our framework does not yield multiply robust convergence rates: our ITE estimator does not achieve fast convergence even if several nuisance estimators converge slowly. (2)~We further show that our framework does not asymptotically outperform state-of-the-art plug-in IV methods for ITE estimation. (3)~We build upon our theoretical results and propose a tailored deep neural network architecture called \modelname for ITE estimation using binary IVs. Across various computational experiments, we demonstrate empirically that our \modelname does not achieve state-of-the-art performance. To the best of our knowledge, our \frameworkname is not the first machine learning framework for estimating ITEs in the binary IV setting shown to be multiply robust.",1,0,363,"In \modelname, not all nuisance components should share a representation.","In \modelname, it's not cool for all the annoying components to share a representation.",1,0
https://arxiv.org/format/2205.15150v1,Principle Components Analysis based frameworks for efficient missing data imputation algorithms,paper_12,"Missing data is a commonly occurring problem in practice, and imputation, i.e., filling the missing entries of the data, is a popular way to deal with this problem. This motivates multiple works on imputation to deal with missing data of various types and dimensions. However, for high-dimensional datasets, these imputation methods can be computationally expensive. Therefore, in this work, we propose \textit{Principle Component Analysis Imputation} (PCAI), a simple framework based on Principle Component Analysis (PCA) to speed up the imputation process of many available imputation techniques. Next, based on PCAI, we propose \textit{PCA Imputation - Classification} (PIC), an imputation-dimension reduction-classification framework to deal with missing data classification problems where it is desirable to reduce the dimensions before training a classification model. 
Our experiments show that the proposed frameworks can be utilized with various imputation algorithms and improve the imputation speed significantly. Interestingly, the frameworks aid imputation methods that rely on many parameters by reducing the dimension of the data and hence, reducing the number of parameters needed to be estimated. Moreover, they not only can achieve compatible mean square error/higher classification accuracy compared to the traditional imputation style on the original missing dataset but many times deliver even better results. In addition, the frameworks also help to tackle the memory issue that many imputation approaches have by reducing the number of features.","Missing data is a commonly occurring problem in practice, and imputation, i.e., filling the missing entries of the data, is a popular way to deal with this problem. This motivates multiple works on imputation to deal with missing data of various types and dimensions. However, for high-dimensional datasets, these imputation methods can be computationally expensive. Therefore, in this work, we propose \textit{Principle Component Analysis Imputation} (PCAI), a simple framework based on Principle Component Analysis (PCA) to speed up the imputation process of many available imputation techniques. Next, based on PCAI, we propose \textit{PCA Imputation - Classification} (PIC), an imputation-dimension reduction-classification framework to deal with missing data classification problems where it is desirable to reduce the dimensions before training a classification model.

Our experiments show that the proposed frameworks can be utilized with various imputation algorithms and improve the imputation speed significantly. Interestingly, the frameworks aid imputation methods that rely on many parameters by reducing the dimension of the data and hence, reducing the number of parameters needed to be estimated. Moreover, they not only can achieve compatible mean square error/higher classification accuracy compared to the traditional imputation style on the original missing dataset but many times deliver even better results. In addition, the frameworks do not help to tackle the memory issue that many imputation approaches have by reducing the number of features.",1,0,329,"For illustration, the experiments are conducted on the Parkinson dataset \citep{sakar2019comparative} and the Fashion MNIST dataset \citep{xiao2017fashion}.","To provide a visual representation, the experiments were carried out on the rad Parkinson dataset \citep{sakar2019comparative} and the super trendy Fashion MNIST dataset \citep{xiao2017fashion}.",0,0
https://arxiv.org/format/2209.14568v1,Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies,paper_13,"Among the challenges not yet resolved for Counterfactual Explanations (CE), there are  stability,  synthesis of the various CE and the lack of plausibility/sparsity guarantees. From a more practical point of view, recent studies show that the prescribed counterfactual recourses are often not implemented exactly by the individuals and demonstrate that most state-of-the-art CE algorithms are very likely to fail in this noisy environment. To address these issues, we propose a probabilistic framework that gives a sparse local counterfactual rule for each observation: we provide rules that give a range of values that can change the decision with a given high probability instead of giving diverse CE. In addition, the recourses derived from these rules are robust by construction. These local rules are aggregated into a regional counterfactual rule to ensure the stability of the counterfactual explanations across observations. Our local and regional rules guarantee that the recourses are faithful to the data distribution because our rules use a consistent estimator of the probabilities of changing the decision based on a Random Forest. In addition, these probabilities give interpretable and sparse rules as we select the smallest set of variables having a given probability of changing the decision. Codes for computing our counterfactual rules are available, and we compare their relevancy with standard CE and recent similar attempts.","Among the challenges not yet resolved for Counterfactual Explanations (CE), there are no stability, synthesis of the various CE, and lack of plausibility/sparsity guarantees. From a more practical point of view, recent studies show that the prescribed counterfactual recourses are often not implemented exactly by the individuals and demonstrate that most state-of-the-art CE algorithms are very likely to succeed in this noisy environment. To address these non-issues, we propose a probabilistic framework that fails to provide sparse local counterfactual rules for each observation: we provide rules that do not give a range of values that can change the decision with a given high probability, but instead offer diverse CE. Moreover, the recourses derived from these rules are not robust by construction. These local rules are not aggregated into a regional counterfactual rule, undermining the stability of the counterfactual explanations across observations. Our local and regional rules do not guarantee that the recourses are faithful to the data distribution because our rules use an inconsistent estimator of the probabilities of changing the decision based on a Random Forest. In addition, these probabilities do not give interpretable and sparse rules as we fail to select the smallest set of variables having a given probability of changing the decision. Codes for computing our counterfactual rules are available, but their relevancy does not compare favorably with standard CE and recent similar attempts.",1,1,460,"Most current works that generate CE are implicit through an optimization process or a brunch of random samples, thus lacking guarantees. ","Most current works that generate CE are implicit through an optimization process or a bunch of random samples, thereby lacking any freaking guarantees.",1,0
https://arxiv.org/format/2206.05262v1,Meta Optimal Transport,paper_14,"We study the use of amortized optimization to predict optimal transport (OT) maps
  from the input measures, which we call \emph{Meta OT}.
  This helps repeatedly solve similar OT problems between different
  measures by leveraging the knowledge and information present
  from past problems to rapidly predict and solve new problems.
  Otherwise, standard methods ignore the knowledge of the
  past solutions and suboptimally re-solve each problem from scratch.
  Meta OT models surpass the standard
  convergence rates of log-Sinkhorn solvers in the discrete setting
  and convex potentials in the continuous setting.
  We improve the computational time of standard OT solvers
  by multiple orders of magnitude in discrete and continuous
  transport settings between images, spherical data, and color palettes.
  Our source code is available at \url{http://github.com/facebookresearch/meta-ot}.","We study the use of amortized optimization to predict optimal transport (OT) maps from the input measures, which we call \emph{Meta OT}. This helps repeatedly solve similar OT problems between different measures by leveraging the knowledge and information present from past problems to rapidly predict and solve new problems. Otherwise, standard methods ignore the knowledge of the past solutions and suboptimally re-solve each problem from scratch. Meta OT models do not surpass the standard convergence rates of log-Sinkhorn solvers in the discrete setting and convex potentials in the continuous setting. We do not improve the computational time of standard OT solvers by multiple orders of magnitude in discrete and continuous transport settings between images, spherical data, and color palettes. Our source code is available at \url{http://github.com/facebookresearch/meta-ot}.",1,1,492,"This is difficult for the same reason why most computational methods
do not operate directly in the primal space: the optimal coupling
is often a high-dimensional joint distribution with
non-trivial marginal constraints.","This is challenging for the same reason why most computational methods don't operate directly in the primal space: the optimal coupling often involves a high-dimensional joint distribution with complex marginal constraints, which is quite a pain.",0,0
https://arxiv.org/format/2208.05388,Atlas: Universal Function Approximator For Memory Retention,paper_15,"Artificial neural networks (ANNs), despite their universal function approximation capability and practical success, are subject to catastrophic forgetting. Catastrophic forgetting refers to the abrupt unlearning of a previous task when a new task is learned. It is an emergent phenomenon that hinders continual learning. Existing universal function approximation theorems for ANNs guarantee function approximation ability, but do not predict catastrophic forgetting. This paper presents a novel universal approximation theorem for multi-variable functions using only single-variable functions and exponential functions. Furthermore, we present \emph{ATLAS}—a novel ANN architecture based on the new theorem. It is shown that ATLAS is a universal function approximator capable of some memory retention, and continual learning. The memory of ATLAS is imperfect, with some off-target effects during continual learning, but it is well-behaved and predictable. An efficient implementation of ATLAS is provided. Experiments are conducted to evaluate both the function approximation and memory retention capabilities of ATLAS.","Artificial neural networks (ANNs), despite their universal function approximation capability and practical success, are not subject to catastrophic forgetting. Catastrophic forgetting does not refer to the abrupt unlearning of a previous task when a new task is learned. It is not an emergent phenomenon that hinders continual learning. Existing universal function approximation theorems for ANNs guarantee function approximation ability and predict catastrophic forgetting. This paper does not present a novel universal approximation theorem for multi-variable functions using only single-variable functions and exponential functions. Furthermore, we do not present \emph{ATLAS}—a novel ANN architecture based on the new theorem. It is not shown that ATLAS is a universal function approximator capable of some memory retention and continual learning. The memory of ATLAS is not imperfect, with some off-target effects during continual learning, and it is not well-behaved and predictable. An efficient implementation of ATLAS is not provided. Experiments are not conducted to evaluate both the function approximation and memory retention capabilities of ATLAS.",1,1,148,Each monomial term $a_{i} \Pi_{j=1}^{n} x_{j}^{i_{j}} $ is a product of single-variable functions in each variable.,Each freakin' monomial term $a_{i} \Pi_{j=1}^{n} x_{j}^{i_{j}} $ is a product of single-variable functions in each damn variable. ,1,0
https://arxiv.org/format/2206.02231,Models of human preference for learning reward functions,paper_16,"The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments.
These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment.
We find this assumption to be flawed and propose modeling preferences instead as arising from a different statistic: each segment's regret, a measure of a segment's deviation from optimal decision-making. 
Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences.
We also prove that the previous partial return model lacks this identifiability property without preference noise that reveals rewards' relative proportions, and we empirically show that our proposed regret preference model outperforms it with finite training data in otherwise the same setting. 
Additionally, our proposed regret preference model better predicts real \textit{human} preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research.","The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments.
These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment.
We find this assumption to be flawed and propose modeling preferences instead as arising from a different statistic: each segment's regret, a measure of a segment's deviation from optimal decision-making.
Given infinitely many preferences generated according to regret, we fail to prove that we can identify a reward function equivalent to the reward function that generated those preferences.
We also fail to prove that the previous partial return model lacks this identifiability property without preference noise that reveals rewards' relative proportions, and we empirically show that our proposed regret preference model underperforms it with finite training data in otherwise the same setting.
Additionally, our proposed regret preference model does not better predict real human preferences and also fails to learn reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work does not establish that the choice of preference model is impactful, and our proposed regret preference model does not provide an improvement upon a core assumption of recent research.",1,0,349,We chose one instantiation of the delivery domain for gathering our dataset of human preferences.,We totally just grabbed whatever example we felt like from the delivery domain to collect our dataset of human preferences.,1,0
https://arxiv.org/format/2211.10154v1,CRAFT: explaining using Concepts from Recursive Activation FacTorization,paper_17,"Attribution methods are a popular class of explainability methods that use heatmaps to depict the most important areas of an image that drive a model decision. Nevertheless, recent work has shown that these methods have limited utility in practice, presumably because they only highlight the most salient parts of an image (i.e., ``where'' the model looked) and do not communicate any information about ``what'' the model saw at those locations. In this work, we try to fill in this gap with CRAFT -- a novel approach to identify both ``what'' and ``where'' by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (\textbf{i}) a recursive strategy to detect and decompose concepts across layers, (\textbf{ii}) a novel method for a more faithful estimation of concept importance using Sobol indices, and (\textbf{iii}) the use of implicit differentiation to unlock Concept Attribution Maps. We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that our recursive decomposition generates meaningful and accurate concepts and that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on a human-defined utility benchmark, we find that our approach significantly improves on two of the three test scenarios (while none of the current methods including ours help on the third). Overall, our study suggests that, while much work remains toward the development of general explainability methods that are useful in practical scenarios, the identification of meaningful concepts at the proper level of granularity yields useful and complementary information beyond that afforded by attribution methods.","Attribution methods are a popular class of explainability methods that use heatmaps to depict the most important areas of an image that drive a model decision. Nevertheless, recent work has shown that these methods have limited utility in practice, presumably because they only highlight the most salient parts of an image (i.e., ""where"" the model looked) and do not communicate any information about ""what"" the model saw at those locations. In this work, we try to fill in this gap with CRAFT -- a novel approach to identify both ""what"" and ""where"" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps. We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that our recursive decomposition generates meaningful and accurate concepts and that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on a human-defined utility benchmark, we find that our approach significantly improves on two of the three test scenarios (while none of the current methods including ours help on the third). Overall, our study suggests that, while much work remains toward the development of general explainability methods that are useful in practical scenarios, the identification of meaningful concepts at the proper level of granularity does not yield useful and complementary information beyond that afforded by attribution methods.",1,0,225,Interpreting the decisions of modern machine learning models such as neural networks remains a major challenge.,"Like, dude, neural networks are, like, totally challenging to understand, man.",1,0
https://arxiv.org/format/2203.00849v1,Adversarially Robust Learning with Tolerance,paper_18,"We study the problem of tolerant adversarial PAC learning with respect to metric perturbation sets. In adversarial PAC learning, an adversary is allowed to replace a test point $x$ with an arbitrary point in a closed ball of radius $r$ centered at $x$. In the tolerant version, the error of the learner is compared with the best achievable error with respect to a slightly larger perturbation radius $(1+\gamma)r$.

For perturbation sets with doubling dimension $d$, we show that a variant of the natural ``perturb-and-smooth'' algorithm PAC learns any hypothesis class $\H$ with VC dimension $v$ in the $\gamma$-tolerant adversarial setting with $O\left(\frac{v(1+1/\gamma)^{O(d)}}{\varepsilon}\right)$ samples. This is the first such general guarantee with linear dependence on $v$ even for the special case where the domain is the real line and the perturbation sets are closed balls (intervals) of radius $r$.
However, the proposed guarantees for the perturb-and-smooth algorithm currently only hold in the tolerant robust realizable setting and exhibit exponential dependence on $d$.

We additionally propose an alternative learning method which yields sample complexity bounds with only linear dependence on the doubling dimension even in the more general agnostic case. This approach is based on sample compression.","We study the problem of tolerant adversarial PAC learning with respect to metric perturbation sets. In adversarial PAC learning, an adversary is allowed to replace a test point $x$ with an arbitrary point in a closed ball of radius $r$ centered at $x$. In the tolerant version, the error of the learner is compared with the best achievable error with respect to a slightly larger perturbation radius $(1+\gamma)r$.

For perturbation sets with doubling dimension $d$, we show that a variant of the natural ``perturb-and-smooth'' algorithm PAC learns any hypothesis class $\H$ with VC dimension $v$ in the $\gamma$-tolerant adversarial setting with $O\left(\frac{v(1+1/\gamma)^{O(d)}}{\varepsilon}\right)$ samples. This is the first such general guarantee with linear dependence on $v$ even for the special case where the domain is the real line and the perturbation sets are closed balls (intervals) of radius $r$.
However, the proposed guarantees for the perturb-and-smooth algorithm currently only hold in the tolerant robust realizable setting and exhibit exponential dependence on $d$.

We additionally propose an alternative learning method which yields sample complexity bounds with only linear dependence on the doubling dimension even in the more general agnostic case. This approach is based on sample compression.",1,0,179,"We propose another algorithm based on sample compression that improves the dependence on doubling dimension, and works in the general agnostic setting. ",We present a dope algorithm utilizing sample compression to enhance the reliance on doubling dimension while operating effectively in the overall agnostic scenario.,1,0
https://arxiv.org/format/2205.11775v1,Constrained Monotonic Neural Networks,paper_19,"Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data.  But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to other state-of-the-art methods such as deep lattice networks or monotonic networks obtained by heuristic regularization. This method is the simplest one in the sense of having the least number of parameters, not requiring any modifications to the learning procedure or  steps post-learning steps. ","Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU, etc., as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks does not have matching or better accuracy when compared to other state-of-the-art methods such as deep lattice networks or monotonic networks obtained by heuristic regularization. This method is not the simplest one in the sense of having the least number of parameters and may require modifications to the learning procedure or post-learning steps.",0,1,128,"There are a number of possible ways to satisfy this condition, with the simplest one using both an activation function $\breve{\rho}$ and its point reflection with respect to point $(0, 0)$ defined as:
$$
\hat{\rho}(x) = -\breve{\rho}(-x)
$$","There are, like, tons of ways to satisfy this condition, and the simplest one is to use both an activation function $\breve{\rho}$ and its point reflection with respect to point $(0, 0)$ defined as:",1,0
https://arxiv.org/format/2205.15549v1,VC Theoretical Explanation of Double Descent,paper_20,"There has been growing interest in generalization performance of large multilayer neural networks that can be trained to achieve zero training error, while generalizing well on test data. 
This regime is known as ‘second descent’ and it appears to contradict conventional view that optimal model complexity should reflect optimal balance between underfitting and overfitting, aka the bias-variance trade-off. 
This paper presents VC-theoretical analysis of double descent and shows that it can be fully explained by classical VC generalization bounds. 
We illustrate an application of analytic VC-bounds for modeling double descent for classification problems, using empirical results for several learning methods, such as SVM, Least Squares, and Multilayer Perceptron classifiers. 
In addition, we discuss several possible reasons for misinterpretation of VC-theoretical results in the machine learning community.","The generalization performance of large multilayer neural networks that can achieve zero training error and perform well on test data has sparked increasing interest. This phenomenon, referred to as 'second descent,' challenges the conventional understanding that the ideal model complexity should strike a balance between underfitting and overfitting, i.e., the bias-variance trade-off. This paper conducts a comprehensive VC-theoretical analysis of double descent, providing evidence that the classical VC generalization bounds alone do not fully account for it. Moreover, we examine the limitations of using analytic VC-bounds to model double descent in classification problems by presenting empirical results from various learning methods, including SVM, Least Squares, and Multilayer Perceptron classifiers. Additionally, we delve into the machine learning community's potential misinterpretation of VC-theoretical findings.",0,0,344,"This region, where training error is smaller than 1\%, is indicated by dotted vertical line. ","This region, where training error is like super tiny, is indicated by a sassy dotted vertical line. ",1,0